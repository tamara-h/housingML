{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multivariate regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing as pre\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the California housing dataset. Originally downloaded from https://github.com/ageron/handson-ml/tree/master/datasets/housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pandas.read_csv('data/housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has some special features which need some attention:\n",
    "1. The variable ocean_proximity is a categorical variable\n",
    "2. There are some missing values in the total_bedrooms column\n",
    "Let us consider how to deal with each of these.\n",
    "\n",
    "First, deal with the categorical variable ocean_proximity. How many distinct values does this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ISLAND', '<1H OCEAN', 'INLAND', 'NEAR BAY', 'NEAR OCEAN'}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(set(housing.ocean_proximity.values))\n",
    "print(len(set(housing.ocean_proximity.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five unique values and so we will replace this with a one-hot vector of length 5 where 10000 corresponds to <1H OCEAN, 01000 is ISLAND etc. This is a standard way to represent categorical factors. The key is that for any entry (row) only one element of this vector should be 1 - hence one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['1h_ocean'] = [1 if i=='<1H OCEAN' else 0 for i in housing.ocean_proximity.values]\n",
    "housing['island'] = [1 if i=='ISLAND' else 0 for i in housing.ocean_proximity.values]\n",
    "housing['inland'] = [1 if i=='INLAND' else 0 for i in housing.ocean_proximity.values]\n",
    "housing['near_ocean'] = [1 if i=='NEAR OCEAN' else 0 for i in housing.ocean_proximity.values]\n",
    "housing['near_bay'] = [1 if i=='NEAR BAY' else 0 for i in housing.ocean_proximity.values]\n",
    "housing.drop(columns=['ocean_proximity'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to look at the missing values in total_bedrooms, of which there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(housing.total_bedrooms.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we deal with this? It depends. There are several strategies one can use.\n",
    "\n",
    "* Replace with the average of the column, but this loses information about correlation\n",
    "* Replace with the values from the nearest neighbour, based on the values of the other variables.\n",
    "* Use some prior knowledge.\n",
    "\n",
    "We will use some prior knowledge that it is highly likely that the number of bedrooms is strongly correlated with the total number of rooms. We'll fit a linear model to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the non-Nan indices\n",
    "notna = housing.total_bedrooms.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656060227407114"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lm.LinearRegression()\n",
    "model.fit(housing.total_rooms.values[notna].reshape(-1,1), housing.total_bedrooms.values[notna].reshape(-1,1))\n",
    "model.score(housing.total_rooms.values[notna].reshape(-1,1), housing.total_bedrooms.values[notna].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a strong prediction so our intuition is correct. Now we predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "isna = housing.total_bedrooms.isna()\n",
    "missing_bedrooms = model.predict(housing.total_rooms.values[isna].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the imputed values into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Can ignore subsequent warning\n",
    "housing.total_bedrooms.loc[isna] = np.squeeze(missing_bedrooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now have a complete dataset. Can we model it? An obvious thing to try first is a linear model, in which the house price is a weighted sum of the inputs; that is:\n",
    "$$y = w_0 + w_1 x_1 + w_2 x_2 + w_M x_M$$\n",
    "Let's assess this via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = housing.values #returns a numpy array\n",
    "std_scaler = pre.StandardScaler()\n",
    "x_scaled = std_scaler.fit_transform(x)\n",
    "df = pandas.DataFrame(x_scaled)\n",
    "df.columns = housing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#house_price = df[\"median_house_value\"]\n",
    "#df.drop(\"median_house_value\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca = PCA(n_components=9)\n",
    "# X is the matrix transposed (n samples on the rows, m features on the columns)\n",
    "#pca.fit(df)\n",
    "#new_df = pca.transform(df)\n",
    "#new_df = pandas.DataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#house_price_df = pandas.DataFrame(house_price)\n",
    "#house_price_df.reset_index(drop=True, inplace=True)\n",
    "#new_df.reset_index(drop=True, inplace=True)\n",
    "#new_df[\"median_house_value\"] = house_price_df\n",
    "#display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 13)\n",
      "(20640, 1)\n",
      "(19640, 13)\n",
      "(19640, 1)\n"
     ]
    }
   ],
   "source": [
    "# First, extract the data into arrays\n",
    "y = df.median_house_value.values.reshape(-1,1)\n",
    "X = df.drop(columns=['median_house_value'], inplace=False).values\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Pull out 1000 values into a holdout set\n",
    "holdout = random.sample(range(0,10640),1000)\n",
    "X_holdout = X[holdout]\n",
    "y_holdout = y[holdout]\n",
    "Xt = np.delete(X, holdout, 0)\n",
    "yt = np.delete(y, holdout, 0)\n",
    "print(Xt.shape)\n",
    "print(yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average: 0.647187893112871\n",
      "Test average: 0.6449187119334369\n"
     ]
    }
   ],
   "source": [
    "Model = lm.LinearRegression()\n",
    "# Have to shuffle the data because it is grouped.\n",
    "train_avg = 0\n",
    "test_avg = 0\n",
    "number_of_runs = 0 \n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "for train_index, test_index in kf.split(Xt):\n",
    "    number_of_runs = number_of_runs +1\n",
    "    \n",
    "    X_train, X_test = Xt[train_index], Xt[test_index]\n",
    "    y_train, y_test = yt[train_index], yt[test_index]\n",
    "    Model.fit(X_train, y_train)\n",
    "    train = Model.score(X_train, y_train)\n",
    "    test = Model.score(X_test, y_test)\n",
    "    train_avg += train\n",
    "    test_avg += test\n",
    "    \n",
    "train_avg = (train_avg/number_of_runs)\n",
    "test_avg = (test_avg/number_of_runs)\n",
    "\n",
    "print(\"Training average: \" + str(train_avg))\n",
    "print(\"Test average: \" + str(test_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores quoted are $R^2$ (coefficient of determination) values which range from 0 to 1. These are OK but there is much room for improvement and we ought to be able to do much better than this. Options that we could try are:\n",
    "\n",
    "* Normalising/rescaling the data so that all variables have similar values?\n",
    "* Expanding the basis to include terms that are non-linear in the variables?\n",
    "* Removing redundant variables from the data - are there any that are correlated with each other?\n",
    "* Regularisation?\n",
    "\n",
    "This will be the task of the group assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training average: 0.7741684453932557\n",
      "Test average: 0.45016798991349666\n"
     ]
    }
   ],
   "source": [
    "steps = [\n",
    "    ('poly', pre.PolynomialFeatures(degree = 3)),\n",
    "    ('model', lm.RidgeCV(alphas=(0.1, 1,10), fit_intercept=True))\n",
    "]\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "train_avg = 0\n",
    "test_avg = 0\n",
    "number_of_runs = 0 \n",
    "for train_index, test_index in kf.split(Xt):\n",
    "  number_of_runs = number_of_runs +1\n",
    "  X_train, X_test = Xt[train_index], Xt[test_index]\n",
    "  y_train, y_test = yt[train_index], yt[test_index]\n",
    "  ridge_pipe = Pipeline(steps)\n",
    "  ridge_pipe.fit(X_train, y_train)\n",
    "  train = ridge_pipe.score(X_train, y_train)\n",
    "  test = ridge_pipe.score(X_test, y_test)\n",
    "  train_avg += train\n",
    "  test_avg += test\n",
    "\n",
    "train_avg = (train_avg/number_of_runs)\n",
    "test_avg = (test_avg/number_of_runs)\n",
    "\n",
    "print(\"Training average: \" + str(train_avg))\n",
    "print(\"Test average: \" + str(test_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
